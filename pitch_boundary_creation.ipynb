{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziyrny7LjIVm"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RtOqSFG-jdv1"
   },
   "outputs": [],
   "source": [
    "# Some env variables\n",
    "\n",
    "# Location to save data\n",
    "STORAGE_DIR = \"/content/drive/MyDrive/ifgan_workdir\"\n",
    "IMAGE_DIR = STORAGE_DIR + \"/stylegan_generated\"\n",
    "\n",
    "# Working directory (where to clone repositories)\n",
    "WORK_DIR = \"/content\"\n",
    "\n",
    "# How many images to generate using StyleGAN\n",
    "NUMBER_OF_IMAGES = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mGW5a82pbVk5",
    "outputId": "c5e35005-4498-4624-f136-a624a65f4f3d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Mount drive folder (if used as storage location)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WeAGdwv8bb_b",
    "outputId": "92cb96fc-10ec-418c-d524-9a69f449cff6"
   },
   "outputs": [],
   "source": [
    "# Use old version of tensorflow supported by StyleGAN1\n",
    "%tensorflow_version 1.x\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KPeVUDrSbeO9",
    "outputId": "c65118d8-bfd8-4307-fbd2-7c81fd33072d"
   },
   "outputs": [],
   "source": [
    "# Clone repositories\n",
    "%cd {WORK_DIR}\n",
    "\n",
    "!rm -rf img2pose\n",
    "!git clone https://github.com/vitoralbiero/img2pose\n",
    "\n",
    "!rm -rf interfacegan\n",
    "!git clone https://github.com/genforce/interfacegan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ljNXrglTSgDK",
    "outputId": "158ab5aa-f340-47da-9cb5-2417ab4fd112"
   },
   "outputs": [],
   "source": [
    "# Get stylegan model for interfacegan\n",
    "%cd {WORK_DIR}/interfacegan\n",
    "!rm -f models/pretrain/stylegan_ffhq.pth\n",
    "\n",
    "!wget https://www.dropbox.com/s/qyv37eaobnow7fu/stylegan_ffhq.pth?dl=1 -O models/pretrain/stylegan_ffhq.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kSYKNFuTs7jv",
    "outputId": "274a78c3-3060-46ba-f7aa-7c73c944cfbf"
   },
   "outputs": [],
   "source": [
    "# img2pose setup\n",
    "%cd {WORK_DIR}/img2pose/Sim3DR/\n",
    "\n",
    "!sh build_sim3dr.sh\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0Xt7_-gRreu"
   },
   "source": [
    "# Generate images using interfacegan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vfwbN7XORx50",
    "outputId": "ea3c8c98-33bb-4589-b267-6ff3b812c2b9"
   },
   "outputs": [],
   "source": [
    "%cd {WORK_DIR}/interfacegan\n",
    "!python generate_data.py -m stylegan_ffhq -o {IMAGE_DIR} -n {NUMBER_OF_IMAGES}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "je4cP6E6swIg"
   },
   "source": [
    "# Run pose estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236,
     "referenced_widgets": [
      "d40f73cedb0e40488c1bd4aef1f09ff7",
      "6f87e8d4bc59422b8f06b88b9feabb2c",
      "698741ec389b4189970ec8ef71f1ba9e",
      "9c5647a04940483f8323ab4550e27053",
      "365207f48731457da9776f0838601fc8",
      "15b9888c3ea94af0848ad238e1fce4c4",
      "cb242f84729b40868cbed64713083008",
      "cba2bf3c93224ef6b9f0d4b570e68451"
     ]
    },
    "id": "-lW1_sPZ04uB",
    "outputId": "e911facc-323f-481c-9232-e338d90a3ba9"
   },
   "outputs": [],
   "source": [
    "# This is a modified script based on: https://github.com/vitoralbiero/img2pose/blob/main/evaluation/jupyter_notebooks/test_own_images.ipynb\n",
    "%cd {WORK_DIR}/img2pose\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.patches as patches\n",
    "from scipy.spatial.transform import Rotation\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "import scipy.io as sio\n",
    "from utils.renderer import Renderer\n",
    "from utils.image_operations import expand_bbox_rectangle\n",
    "from utils.pose_operations import get_pose\n",
    "from img2pose import img2poseModel\n",
    "from model_loader import load_model\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Create the renderer for visualization (skip?)\n",
    "renderer = Renderer(\n",
    "    vertices_path=\"pose_references/vertices_trans.npy\", \n",
    "    triangles_path=\"pose_references/triangles.npy\"\n",
    ")\n",
    "\n",
    "# Load model weights\n",
    "threed_points = np.load('pose_references/reference_3d_68_points_trans.npy')\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "DEPTH = 18\n",
    "MAX_SIZE = 1400\n",
    "MIN_SIZE = 600\n",
    "\n",
    "POSE_MEAN = STORAGE_DIR + \"/img2pose_models/WIDER_train_pose_mean_v1.npy\"\n",
    "POSE_STDDEV = STORAGE_DIR + \"/img2pose_models/WIDER_train_pose_stddev_v1.npy\"\n",
    "MODEL_PATH = STORAGE_DIR + \"/img2pose_models/img2pose_v1.pth\"\n",
    "\n",
    "pose_mean = np.load(POSE_MEAN)\n",
    "pose_stddev = np.load(POSE_STDDEV)\n",
    "\n",
    "img2pose_model = img2poseModel(\n",
    "    DEPTH, MIN_SIZE, MAX_SIZE, \n",
    "    pose_mean=pose_mean, pose_stddev=pose_stddev,\n",
    "    threed_68_points=threed_points,\n",
    ")\n",
    "load_model(img2pose_model.fpn_model, MODEL_PATH, cpu_mode=str(img2pose_model.device) == \"cpu\", model_only=True)\n",
    "img2pose_model.evaluate()\n",
    "\n",
    "# Load latent vectors - we need to drop any we can't compute the pose for\n",
    "latent_vectors_in_z = np.load(os.path.join(IMAGE_DIR, \"z.npy\"))\n",
    "latent_vectors_in_w = np.load(os.path.join(IMAGE_DIR, \"w.npy\"))\n",
    "latent_vectors_out_z = np.empty((0,512))\n",
    "latent_vectors_out_w = np.empty((0,512))\n",
    "\n",
    "# Run the estimation\n",
    "\n",
    "threshold = 0.9\n",
    "\n",
    "img_paths = [os.path.join(IMAGE_DIR, img_path) for img_path in os.listdir(IMAGE_DIR)]\n",
    "\n",
    "# Uncomment to test quickly on 10 images\n",
    "# img_paths = img_paths[1:100]\n",
    "\n",
    "ifgan_data = np.empty((0,1))\n",
    "all_angles = np.empty((0,6))\n",
    "\n",
    "for img_path in tqdm(img_paths):\n",
    "  if img_path.endswith(\".jpg\"):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    \n",
    "    (w, h) = img.size\n",
    "    image_intrinsics = np.array([[w + h, 0, w // 2], [0, w + h, h // 2], [0, 0, 1]])\n",
    "            \n",
    "    res = img2pose_model.predict([transform(img)])[0]\n",
    "\n",
    "    all_bboxes = res[\"boxes\"].cpu().numpy().astype('float')\n",
    "            \n",
    "    image_name = os.path.split(img_path)[1]\n",
    "    image_name_noext = os.path.splitext(image_name)[0]\n",
    "    image_number = int(image_name_noext)\n",
    "\n",
    "    best_score = 0;\n",
    "    best_pose_pred = None\n",
    "\n",
    "    for i in range(len(all_bboxes)):\n",
    "      current_score = res[\"scores\"][i] \n",
    "      if current_score > threshold and current_score > best_score:\n",
    "        best_pose_pred = res[\"dofs\"].cpu().numpy()[i].astype('float').squeeze()\n",
    "        best_score = current_score\n",
    "    \n",
    "    if best_pose_pred is not None:\n",
    "        image_direction = 0 if best_pose_pred[0] >= 0 else 1\n",
    "\n",
    "        ifgan_data = np.append(ifgan_data, [[image_direction]], 0)\n",
    "        latent_vectors_out_z = np.append(latent_vectors_out_z, [latent_vectors_in_z[image_number]], 0)\n",
    "        latent_vectors_out_w = np.append(latent_vectors_out_w, [latent_vectors_in_w[image_number]], 0)\n",
    "        all_angles = np.append(all_angles, [best_pose_pred], 0)\n",
    "\n",
    "# These are the scores used to create boundaries\n",
    "np.save(STORAGE_DIR + \"/face_pitch_scores.npy\", ifgan_data)\n",
    "\n",
    "# z space and w space latent vectors (these should have the same amount of samples as there are scores in ifgan_data)\n",
    "np.save(STORAGE_DIR + \"/face_pitch_latents_z.npy\", latent_vectors_out_z)\n",
    "np.save(STORAGE_DIR + \"/face_pitch_latents_w.npy\", latent_vectors_out_w)\n",
    "\n",
    "# All angles and translations predicted by img2pose (for stats and later use)\n",
    "np.save(STORAGE_DIR + \"/all_face_angles.npy\", all_angles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOo75nYGhctA"
   },
   "source": [
    "# Create boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GEodwAPkThBz"
   },
   "outputs": [],
   "source": [
    "# What thresholds to use when filtering & generating boundaries\n",
    "degree_thresholds = [0, 5, 10, 15, 20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LVdVRZnjyfJX"
   },
   "outputs": [],
   "source": [
    "%cd {WORK_DIR}/interfacegan\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "import math\n",
    "from IPython.display import Image\n",
    "\n",
    "def ShowTable(data):\n",
    "  print(pandas.DataFrame(data))\n",
    "\n",
    "scores = np.load(STORAGE_DIR + \"/face_pitch_scores.npy\")\n",
    "latent_vectors_z = np.load(STORAGE_DIR + \"/face_pitch_latents_z.npy\")\n",
    "latent_vectors_w = np.load(STORAGE_DIR + \"/face_pitch_latents_w.npy\")\n",
    "face_angles = np.load(STORAGE_DIR + \"/all_face_angles.npy\")\n",
    "\n",
    "pitches = face_angles[:,0]\n",
    "\n",
    "print(\"Total looking up: \", np.sum(pitches < 0))\n",
    "print(\"Total looking down: \", np.sum(pitches >= 0))\n",
    "\n",
    "# Average pitch angle\n",
    "print(np.average(pitches))\n",
    "\n",
    "\n",
    "# Find min and max angle\n",
    "print(\"Max angle (down): \", math.degrees(np.max(pitches)))\n",
    "print(\"Min angle (up)  : \", math.degrees(np.min(pitches)))\n",
    "\n",
    "# Filter samples based on thresholds and create separate files\n",
    "for threshold in degree_thresholds:\n",
    "  radian_threshold = math.radians(threshold)\n",
    "\n",
    "  filtered_scores = np.empty((0,1))\n",
    "  filtered_latent_z = np.empty((0,512))\n",
    "  filtered_latent_w = np.empty((0,512))\n",
    "\n",
    "  for idx, face_angle in enumerate(face_angles):\n",
    "    if abs(face_angle[0]) > radian_threshold:\n",
    "      filtered_scores = np.append(filtered_scores, [scores[idx]], 0)\n",
    "      filtered_latent_z = np.append(filtered_latent_z, [latent_vectors_z[idx]], 0)\n",
    "      filtered_latent_w = np.append(filtered_latent_w, [latent_vectors_w[idx]], 0)\n",
    "\n",
    "  print(\"Threshold: \", str(threshold))\n",
    "  print(\"Samples:   \" + str(len(filtered_scores)))\n",
    "  print(\"Looking up:  \" + str(np.sum(filtered_scores == 0)))\n",
    "  print(\"Looking down:  \" + str(np.sum(filtered_scores == 1)))\n",
    "\n",
    "  base_filepath = STORAGE_DIR + \"/filter_\" + str(threshold)\n",
    "  np.save(base_filepath + \"_face_pitch_scores.npy\", filtered_scores)\n",
    "  np.save(base_filepath + \"_face_pitch_latents_z.npy\", filtered_latent_z)\n",
    "  np.save(base_filepath + \"_face_pitch_latents_w.npy\", filtered_latent_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VA8M8o-2hfvI"
   },
   "outputs": [],
   "source": [
    "# Generate boundaries for each threshold\n",
    "\n",
    "%cd {WORK_DIR}/interfacegan\n",
    "\n",
    "OUT_DIR = \"boundaries/stylegan_ffqh_pitch\"\n",
    "\n",
    "!rm -rf {OUT_DIR}\n",
    "\n",
    "for threshold in degree_thresholds:\n",
    "  !python train_boundary.py \\\n",
    "      -o {OUT_DIR}_{threshold} \\\n",
    "      -c {STORAGE_DIR}/filter_{threshold}_face_pitch_latents_w.npy \\\n",
    "      -s {STORAGE_DIR}/filter_{threshold}_face_pitch_scores.npy \\\n",
    "      -n 0.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RR8iMSGalZV_"
   },
   "source": [
    "# Test boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4biBoG2jWIH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "num_samples = 5\n",
    "all_w_samples = np.load(os.path.join(IMAGE_DIR, \"w.npy\"))\n",
    "my_w_samples = np.empty((num_samples,512))\n",
    "\n",
    "# Pull some random W samples\n",
    "for i in range(0, num_samples):\n",
    "  sampleIndex = random.randint(0, NUMBER_OF_IMAGES-1)\n",
    "  my_w_samples[i] = all_w_samples[sampleIndex]\n",
    "\n",
    "print(my_w_samples)\n",
    "np.save(STORAGE_DIR + \"/random_w_samples.npy\", my_w_samples);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahy_7jRIq-2s"
   },
   "outputs": [],
   "source": [
    "# Generate images based on all samples from the previous step, and all thresholds\n",
    "%cd {WORK_DIR}/interfacegan\n",
    "\n",
    "for threshold in degree_thresholds:\n",
    "  !python edit.py \\\n",
    "      -m stylegan_ffhq \\\n",
    "      -b boundaries/stylegan_ffqh_pitch_{threshold}/boundary.npy \\\n",
    "      -i {STORAGE_DIR}/random_w_samples.npy \\\n",
    "      -o {STORAGE_DIR}/pitch_editing_degthresh_{threshold} \\\n",
    "      -s 'w' \\\n",
    "      --start_distance -2.0 \\\n",
    "      --end_distance 2.0 \\\n",
    "      --steps 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLm26CZacXuw"
   },
   "outputs": [],
   "source": [
    "# Combine generated images into 1 for each sample\n",
    "from PIL import Image\n",
    "\n",
    "for subject in range(0, 5):\n",
    "  blank_image = Image.new(\"RGB\", (1000, 500))\n",
    "  \n",
    "  for idy, threshold in enumerate(degree_thresholds):\n",
    "    fname = STORAGE_DIR + \"/pitch_editing_degthresh_\" + str(threshold)\n",
    "    for idx in range(0, 10):\n",
    "      iname = fname + \"/\" + str(subject).zfill(3) + \"_\" + str(idx).zfill(3) + \".jpg\" \n",
    "      image = Image.open(iname)\n",
    "      blank_image.paste(image.resize((100, 100)), (idx*100,idy*100))  \n",
    "  \n",
    "  display(blank_image)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNfeEyDLbkedP8GREmzD/BZ",
   "collapsed_sections": [
    "ziyrny7LjIVm",
    "-0Xt7_-gRreu",
    "je4cP6E6swIg"
   ],
   "name": "semantic_face_editing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "15b9888c3ea94af0848ad238e1fce4c4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "365207f48731457da9776f0838601fc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "698741ec389b4189970ec8ef71f1ba9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_15b9888c3ea94af0848ad238e1fce4c4",
      "max": 10004,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_365207f48731457da9776f0838601fc8",
      "value": 10004
     }
    },
    "6f87e8d4bc59422b8f06b88b9feabb2c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c5647a04940483f8323ab4550e27053": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cba2bf3c93224ef6b9f0d4b570e68451",
      "placeholder": "​",
      "style": "IPY_MODEL_cb242f84729b40868cbed64713083008",
      "value": " 10004/10004 [34:42&lt;00:00,  4.80it/s]"
     }
    },
    "cb242f84729b40868cbed64713083008": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cba2bf3c93224ef6b9f0d4b570e68451": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d40f73cedb0e40488c1bd4aef1f09ff7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_698741ec389b4189970ec8ef71f1ba9e",
       "IPY_MODEL_9c5647a04940483f8323ab4550e27053"
      ],
      "layout": "IPY_MODEL_6f87e8d4bc59422b8f06b88b9feabb2c"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
